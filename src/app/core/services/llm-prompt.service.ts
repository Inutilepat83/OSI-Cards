import { Injectable } from '@angular/core';
import { HttpClient } from '@angular/common/http';
import { Observable, of } from 'rxjs';
import { catchError, map } from 'rxjs/operators';

/**
 * Service to serve the LLM prompt for OSI Cards
 * The prompt is pre-generated during build and served as a static JSON file
 * This ensures consistency between documentation and API endpoint
 *
 * The prompt is generated by scripts/generate-llm-prompt-json.js which
 * reuses the same generator from scripts/generate-llm-prompt.js
 */
@Injectable({
  providedIn: 'root',
})
export class LlmPromptService {
  private readonly PROMPT_JSON_PATH = '/assets/api/llm_prompt.json';

  constructor(private http: HttpClient) {}

  /**
   * Get the LLM prompt as a string
   */
  getPrompt(): Observable<string> {
    return this.getPromptJson().pipe(map((response) => response.prompt));
  }

  /**
   * Get the LLM prompt as JSON response
   * Loads the pre-generated prompt JSON file
   */
  getPromptJson(): Observable<{
    prompt: string;
    generatedAt: string;
    version: string;
    metadata?: any;
  }> {
    return this.http.get<any>(this.PROMPT_JSON_PATH).pipe(
      catchError((error) => {
        console.error('Error loading LLM prompt JSON:', error);
        return of({
          prompt: this.getFallbackPrompt(),
          generatedAt: new Date().toISOString(),
          version: '1.0.0',
        });
      })
    );
  }

  /**
   * Fallback prompt if JSON file cannot be loaded
   */
  private getFallbackPrompt(): string {
    return `You are a JSON generator for OSI Cards. Generate valid JSON configurations for card UI components.

CRITICAL RULES:
1. Return ONLY valid JSON - no markdown, no explanations
2. cardTitle and sections are REQUIRED
3. Each section needs title and type
4. Use correct data structure: fields, items, or chartData based on type

For complete documentation, visit: https://osi-card.web.app/docs/llm-integration`;
  }
}
