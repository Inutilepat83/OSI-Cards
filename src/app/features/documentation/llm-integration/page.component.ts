import { Component, ChangeDetectionStrategy } from '@angular/core';
import { DocPageComponent } from '../doc-page.component';

const pageContent = `# LLM Integration

Guide for integrating OSI Cards with Large Language Models.

## Overview

OSI Cards is designed to render structured data generated by LLMs. This guide covers best practices for prompt engineering and handling LLM responses.

## Prompt Template

Include the schema in your LLM prompt:

\`\`\`typescript
const systemPrompt = \\\`
You are a helpful assistant that generates structured card data.
Return JSON matching the following TypeScript interface:

interface AICardConfig {
  cardTitle: string;
  sections: CardSection[];
}

interface CardSection {
  title: string;
  type: 'info' | 'analytics' | 'contact-card' | 'list' | 'chart';
  fields?: Array<{ label: string; value: string }>;
  metrics?: Array<{ label: string; value: string; trend?: 'up' | 'down' }>;
  items?: Array<{ title: string; description?: string }>;
}

Return ONLY valid JSON, no markdown or explanation.
\\\`;
\`\`\`

## Streaming Integration

Handle streaming responses from LLMs:

\`\`\`typescript
async function streamCard(prompt: string) {
  const response = await fetch('/api/chat', {
    method: 'POST',
    body: JSON.stringify({ prompt }),
    headers: { 'Content-Type': 'application/json' }
  });
  
  const reader = response.body.getReader();
  let buffer = '';
  
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    
    buffer += new TextDecoder().decode(value);
    
    try {
      const partialCard = JSON.parse(buffer);
      streamingService.updateCard(partialCard);
    } catch {
      // Buffer incomplete, continue reading
    }
  }
}
\`\`\`

## Validation

Always validate LLM output:

\`\`\`typescript
import { isValidCardConfig, sanitizeCardConfig } from 'osi-cards-lib';

function processLLMResponse(json: string): AICardConfig | null {
  try {
    const data = JSON.parse(json);
    
    if (!isValidCardConfig(data)) {
      console.error('Invalid card config from LLM');
      return null;
    }
    
    return sanitizeCardConfig(data);
  } catch (error) {
    console.error('Failed to parse LLM response:', error);
    return null;
  }
}
\`\`\`

## Error Handling

\`\`\`typescript
const fallbackCard: AICardConfig = {
  cardTitle: 'Error',
  sections: [{
    title: 'Unable to generate card',
    type: 'info',
    fields: [{ label: 'Status', value: 'Please try again' }]
  }]
};

streamingService.streamCard(prompt).pipe(
  catchError(error => {
    console.error('Streaming error:', error);
    return of(fallbackCard);
  })
).subscribe(card => this.cardConfig = card);
\`\`\`

## Best Practices

1. **Use structured output modes** - Many LLMs support JSON mode
2. **Include examples** - Few-shot prompting improves quality
3. **Validate aggressively** - Never trust LLM output blindly
4. **Handle partial JSON** - Streaming may produce incomplete JSON
5. **Set timeouts** - Prevent hanging on slow responses
`;

@Component({
  selector: 'app-llm-integration-page',
  standalone: true,
  imports: [DocPageComponent],
  template: `<app-doc-page [content]="content"></app-doc-page>`,
  changeDetection: ChangeDetectionStrategy.OnPush
})
export class LlmIntegrationPageComponent {
  content = pageContent;
}

export default LlmIntegrationPageComponent;
